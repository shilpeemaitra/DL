{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Implement **L1, L2, and ElasticNet** regularization techniques on a single-layered neural network with four input nodes and a bias. Assume the input vector has the values [0.5, 1.5, 1.0, 0.5] and a bias of -1, with no activation function at the output node. The learned parameter vector W = [0.1, 0.2, 0.3, 0.4] should return the expected result of \"1.75.\"\n",
        "\n",
        "\n",
        "a) Write a Python function to estimate the total loss using L1, L2, and ElasticNet regularization separately. The loss function should be defined as the mean squared error between the predicted output and the expected result. Implement regularization terms for L1, L2, and ElasticNet with regularization strengths (alpha) of 0.01, 0.05, and 0.1, respectively, and **compute the total loss for each regularization technique**.\n"
      ],
      "metadata": {
        "id": "WPL7CnhJfVQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Defining all the values\n",
        "\n",
        "X = np.array([0.5, 1.5, 1.0, 0.5])\n",
        "bias = -1\n",
        "expected_result = 1.75\n",
        "W = np.array([0.1, 0.2, 0.3, 0.4])\n",
        "\n",
        "\n",
        "def mse_loss(y_pred, y_true):\n",
        "    return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "#  L1\n",
        "def l1_regularization(W, alpha):\n",
        "    return alpha * np.sum(np.abs(W))\n",
        "\n",
        "# L2\n",
        "def l2_regularization(W, alpha):\n",
        "    return 0.5 * alpha * np.sum(W ** 2)\n",
        "\n",
        "#  ELASTICNET\n",
        "def elasticnet_regularization(W, alpha, rho):\n",
        "    return alpha * ((1 - rho) * np.sum(np.abs(W)) + 0.5 * rho * np.sum(W ** 2))\n",
        "\n",
        "\n",
        "\n",
        "#  to calculate total loss with\n",
        "def total_loss(X, W, bias, expected_result, alpha, regularization_type, rho=None):\n",
        "    y_pred = np.dot(X, W) + bias\n",
        "    loss = mse_loss(y_pred, expected_result)\n",
        "\n",
        "    if regularization_type == 'L1':\n",
        "        regularization_term = l1_regularization(W, alpha)\n",
        "    elif regularization_type == 'L2':\n",
        "        regularization_term = l2_regularization(W, alpha)\n",
        "    elif regularization_type == 'ElasticNet':\n",
        "        regularization_term = elasticnet_regularization(W, alpha, rho)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid regularization type\")\n",
        "\n",
        "    total_loss = loss + regularization_term\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "\n",
        "alpha_values = [0.01, 0.05, 0.1]\n",
        "regularization_types = ['L1', 'L2', 'ElasticNet']\n",
        "rho = 0.5  # Rho value\n",
        "\n",
        "for alpha in alpha_values:\n",
        "    for regularization_type in regularization_types:\n",
        "        loss = total_loss(X, W, bias, expected_result, alpha, regularization_type, rho)\n",
        "        print(f\"Total loss with {regularization_type} regularization (alpha={alpha}): {loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH9UMZAMYdF8",
        "outputId": "69c1794f-7ecc-4c75-a163-96f808be1dd9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total loss with L1 regularization (alpha=0.01): 3.6199999999999997\n",
            "Total loss with L2 regularization (alpha=0.01): 3.6115\n",
            "Total loss with ElasticNet regularization (alpha=0.01): 3.61575\n",
            "Total loss with L1 regularization (alpha=0.05): 3.6599999999999997\n",
            "Total loss with L2 regularization (alpha=0.05): 3.6174999999999997\n",
            "Total loss with ElasticNet regularization (alpha=0.05): 3.63875\n",
            "Total loss with L1 regularization (alpha=0.1): 3.71\n",
            "Total loss with L2 regularization (alpha=0.1): 3.625\n",
            "Total loss with ElasticNet regularization (alpha=0.1): 3.6675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Compare** the effects of L1, L2, and ElasticNet regularization techniques by analyzing their impact on the total loss and the learned parameter vector.Discuss the strengths and weaknesses of each regularization technique in terms of controlling model complexity and preventing overfitting\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "R6YdBR8vbpXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**L1 (Lasso) regularization technique:**\n",
        "\n",
        "\n",
        "reduces the sparsity and brings all the parameters weights down to exactly zero. total loss will increase with the regularization strength.\n",
        "\n",
        "l1 works good in high dimensional data with many features where most of them are irrelevant.\n",
        "\n",
        "effective with huge data\n",
        "may remove useful features too aggresively.\n",
        "\n",
        "\n",
        "\n",
        "**L2 (Ridge) regularization technique:**\n",
        "\n",
        "total loss value increases as the weight increases\n",
        "\n",
        "helps with faster convergence in training because of the smooth updates.\n",
        "\n",
        "helps prevent overfitting and does not delete the weights completely just reduces it.\n",
        "\n",
        "may not work well in instances where some fearures are more important than others.\n",
        "\n",
        "\n",
        "\n",
        "**Elastic net**:\n",
        "\n",
        "this is a combination of both\n",
        "l1 and l2\n",
        "\n",
        "The total loss increases based on the combined effect of L1 and L2\n",
        "\n",
        "\n",
        "It can handle situations where L1 alone might be too aggressive by also penalizing large weights with L2 regularization\n",
        "\n",
        "\n",
        " Suitable for datasets with correlated features\n",
        "\n",
        " May still discard potentially useful features if L1 penalty dominates\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iAUiikFzbWtB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##c)\n",
        "\n",
        " Implement **a multi-layer perceptron** (MLP) to approximate the XOR function using only numpy without using any built-in datasets or libraries. Design a network with one hidden layer containing two neurons and an output layer. Initialize the weights and biases randomly. Train the network using backpropagation for the XOR function with input values (0,0),(0,1),(1,0),(1,1) and target outputs 0,1,1,0, respectively.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pJEDC9Ske22I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "#derivative of the sigmoid function\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "\n",
        "\n",
        "X = np.array([[0, 0],\n",
        "              [0, 1],\n",
        "              [1, 0],\n",
        "              [1, 1]])\n",
        "\n",
        "y = np.array([[0],\n",
        "              [1],\n",
        "              [1],\n",
        "              [0]])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "np.random.seed(1)\n",
        "input_neurons = 2\n",
        "hidden_neurons = 2\n",
        "output_neurons = 1\n",
        "\n",
        "weights_input_hidden = np.random.uniform(size=(input_neurons, hidden_neurons))\n",
        "biases_hidden = np.random.uniform(size=(1, hidden_neurons))\n",
        "\n",
        "weights_hidden_output = np.random.uniform(size=(hidden_neurons, output_neurons))\n",
        "biases_output = np.random.uniform(size=(1, output_neurons))\n",
        "\n",
        "learning_rate = 0.5\n",
        "epochs = 10000\n",
        "\n",
        "#backpropagation\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    hidden_layer_input = np.dot(X, weights_input_hidden) + biases_hidden\n",
        "    hidden_layer_output = sigmoid(hidden_layer_input)\n",
        "\n",
        "    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + biases_output\n",
        "    predicted_output = sigmoid(output_layer_input)\n",
        "\n",
        "\n",
        "    error = y - predicted_output\n",
        "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
        "\n",
        "    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n",
        "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
        "    biases_output += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    weights_input_hidden += X.T.dot(d_hidden_layer) * learning_rate\n",
        "    biases_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "# final predictions\n",
        "print(\"Final predictions:\")\n",
        "print(predicted_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVVEG2Ihe8SA",
        "outputId": "13ce5953-dbfe-4811-eaf9-8fe9de2b5639"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final predictions:\n",
            "[[0.01931569]\n",
            " [0.98331818]\n",
            " [0.9833515 ]\n",
            " [0.01725879]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given **a convolutional neural network (CNN)**  architecture with one convolutional layer followed by a max-pooling layer and a fully connected layer, compute the total number of parameters in the network. Assume the input image size is 32×32, the convolutional layer has 16 filters of size 3×3, the max-pooling layer has a pool size of 2×2, and the fully connected layer has 256 neurons. (10 Marks)\n",
        "\n"
      ],
      "metadata": {
        "id": "k80xdrD0jOaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conv_filters = 16\n",
        "conv_filter_size = 3\n",
        "conv_input_channels = 3\n",
        "conv_bias = 1\n",
        "\n",
        "\n",
        "conv_params = conv_filters * (conv_filter_size * conv_filter_size * conv_input_channels + conv_bias)\n",
        "fc_neurons = 256\n",
        "fc_input_size = 16 * 16 * 16\n",
        "fc_bias = 1\n",
        "\n",
        "fc_params = fc_neurons * (fc_input_size + fc_bias)\n",
        "\n",
        "total_params = conv_params + fc_params\n",
        "print(\"Total number of parameters in the network:\", total_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7bpJCACgkNw",
        "outputId": "9113cedf-3e6d-427a-c493-4dfbbdb04ae9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters in the network: 1049280\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define input vector, bias, expected result, and learned parameter vector\n",
        "X = np.array([0.5, 1.5, 1.0, 0.5])\n",
        "bias = -1\n",
        "expected_result = 1.75\n",
        "W = np.array([0.1, 0.2, 0.3, 0.4])\n",
        "\n",
        "# Function to calculate mean squared error loss\n",
        "def mse_loss(y_pred, y_true):\n",
        "    return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "# Function to calculate L1 regularization term\n",
        "def l1_regularization(W, alpha):\n",
        "    return alpha * np.sum(np.abs(W))\n",
        "\n",
        "# Function to calculate L2 regularization term\n",
        "def l2_regularization(W, alpha):\n",
        "    return 0.5 * alpha * np.sum(W ** 2)\n",
        "\n",
        "# Function to calculate ElasticNet regularization term\n",
        "def elasticnet_regularization(W, alpha, rho):\n",
        "    return alpha * ((1 - rho) * np.sum(np.abs(W)) + 0.5 * rho * np.sum(W ** 2))\n",
        "\n",
        "# Function to calculate total loss with regularization\n",
        "def total_loss(X, W, bias, expected_result, alpha, regularization_type, rho=None):\n",
        "    y_pred = np.dot(X, W) + bias\n",
        "    loss = mse_loss(y_pred, expected_result)\n",
        "\n",
        "    if regularization_type == 'L1':\n",
        "        regularization_term = l1_regularization(W, alpha)\n",
        "    elif regularization_type == 'L2':\n",
        "        regularization_term = l2_regularization(W, alpha)\n",
        "    elif regularization_type == 'ElasticNet':\n",
        "        regularization_term = elasticnet_regularization(W, alpha, rho)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid regularization type\")\n",
        "\n",
        "    total_loss = loss + regularization_term\n",
        "    return total_loss\n",
        "\n",
        "# Test the function for each regularization technique\n",
        "alpha_values = [0.01, 0.05, 0.1]\n",
        "regularization_types = ['L1', 'L2', 'ElasticNet']\n",
        "rho = 0.5  # Rho value for ElasticNet\n",
        "\n",
        "for alpha in alpha_values:\n",
        "    for regularization_type in regularization_types:\n",
        "        loss = total_loss(X, W, bias, expected_result, alpha, regularization_type, rho)\n",
        "        print(f\"Total loss with {regularization_type} regularization (alpha={alpha}): {loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH9UMZAMYdF8",
        "outputId": "b4989569-1a52-4670-b0df-927b2e72e4c3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total loss with L1 regularization (alpha=0.01): 3.6199999999999997\n",
            "Total loss with L2 regularization (alpha=0.01): 3.6115\n",
            "Total loss with ElasticNet regularization (alpha=0.01): 3.61575\n",
            "Total loss with L1 regularization (alpha=0.05): 3.6599999999999997\n",
            "Total loss with L2 regularization (alpha=0.05): 3.6174999999999997\n",
            "Total loss with ElasticNet regularization (alpha=0.05): 3.63875\n",
            "Total loss with L1 regularization (alpha=0.1): 3.71\n",
            "Total loss with L2 regularization (alpha=0.1): 3.625\n",
            "Total loss with ElasticNet regularization (alpha=0.1): 3.6675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Compare** the effects of L1, L2, and ElasticNet regularization techniques by analyzing their impact on the total loss and the learned parameter vector.\n",
        "\n",
        "Discuss the strengths and weaknesses of each regularization technique in terms of controlling model complexity and preventing overfitting\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "R6YdBR8vbpXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**L1 (Lasso) regularization technique:**\n",
        "\n",
        "\n",
        "reduces the sparsity and brings all the parameters weights down to exactly zero. total loss will increase with the regularization strength.\n",
        "\n",
        "l1 works good in high dimensional data with many features where most of them are irrelevant.\n",
        "\n",
        "effective with huge data\n",
        "may remove useful features too aggresively.\n",
        "\n",
        "\n",
        "\n",
        "**L2 (Ridge) regularization technique:**\n",
        "\n",
        "total loss value increases as the weight increases\n",
        "\n",
        "helps with faster convergence in training because of the smooth updates.\n",
        "\n",
        "helps prevent overfitting and does not delete the weights completely just reduces it.\n",
        "\n",
        "may not work well in instances where some fearures are more important than others.\n",
        "\n",
        "\n",
        "\n",
        "**Elastic net**:\n",
        "\n",
        "this is a combination of both\n",
        "l1 and l2\n",
        "\n",
        "The total loss increases based on the combined effect of L1 and L2\n",
        "\n",
        "\n",
        "It can handle situations where L1 alone might be too aggressive by also penalizing large weights with L2 regularization\n",
        "\n",
        "\n",
        " Suitable for datasets with correlated features\n",
        "\n",
        " May still discard potentially useful features if L1 penalty dominates\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iAUiikFzbWtB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YyUSx_wwZQpx"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}